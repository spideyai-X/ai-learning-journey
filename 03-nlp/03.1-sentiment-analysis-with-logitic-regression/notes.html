<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sentiment Analysis with Logistic Regression Notes</title>

    <link rel="stylesheet" href="../../styles/main.css">
    
    <link rel="stylesheet" href="../../styles/katex.min.css">
    <script defer src="../../styles/katex.min.js"></script>
    <script defer src="../../styles/auto-render.min.js"></script>

    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '\\[', right: '\\]', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false}
                ]
            });
        });
    </script>
</head>
<body>
    <div class="container">
        <p>My goal in this section is to build a sentiment analysis model using Logistic Regression. This model will classify tweets as either positive or negative.</p>

        <h1>1. The Supervised Learning Framework</h1>
        <p>The core idea is to train a model using labeled data. In our case, the features $X$ are the tweets, and the labels $Y$ are their sentiments (1 for positive, 0 for negative).</p>
        <p>The training process is an iterative loop:</p>
        <ol>
            <li>Input features $X$ into a prediction function.</li>
            <li>The function, using its current parameters $\theta$, generates a prediction $\hat{Y}$.</li>
            <li>We calculate the `Cost`, which measures how far $\hat{Y}$ is from the actual label $Y$.</li>
            <li>We adjust the parameters $\theta$ (using an optimizer like Gradient Descent) to minimize this cost.</li>
        </ol>
        <img src="./images/supervised-ml-training.png" alt="Supervised ML Training">

        <h1>2. Feature Extraction: From Text to Numbers</h1>
        <p>A model can't understand raw text. We need to convert each tweet into a numerical vector.</p>

        <h3>Method 1: Sparse Representation (Bag of Words)</h3>
        <p>The most straightforward way is to create a feature vector for each tweet based on a vocabulary $V$ (the set of all unique words across all tweets, our corpus).</p>
        <p>For a single tweet, the feature vector $x$ would have the size of $V$. Each element of $x$ is 1 if the corresponding word from $V$ is in the tweet, and 0 otherwise. This can be expressed as:
        $$x_i = \begin{cases} 1 & \text{if word } i \in \text{tweet} \\ 0 & \text{otherwise} \end{cases}$$
        </p>
        <img src="./images/feature-extraction.png" alt="Feature extraction">
        <p>This is called a <strong>sparse representation</strong> because for a large vocabulary, the vector will be mostly zeros.</p>
        <p><strong>Problem</strong>: If $V$ contains 10,000 words, our model needs to train on 10,001 parameters ($n+1$, where $n$ is the size of $V$ and $+1$ is for the bias term). This can be computationally very expensive.</p>
        <img src="./images/problem-with-sparse-representation.png" alt="Problem with sparse representation">

        <h3>Method 2: Feature Engineering with Word Frequencies</h3>
        <p>To avoid large vectors, we can engineer more meaningful features. Instead of a huge sparse vector, we can represent each tweet with a dense vector of only 3 features.</p>
        <p>First, we pre-calculate a frequency map (`freqs`) for every word in our vocabulary $V$. This map stores how many times each word appears in positive tweets versus negative tweets.</p>
        <img src="./images/word-frequency.png" alt="Word frequency">
        <p>Now, for any given tweet $m$, we can build its feature vector $X_m$ as follows:</p>
        <ol>
            <li><strong>Feature 1 (Bias Unit)</strong>: Always `1`.</li>
            <li><strong>Feature 2 (Positive Score)</strong>: The sum of the positive frequencies for every unique word in the tweet.</li>
            <li><strong>Feature 3 (Negative Score)</strong>: The sum of the negative frequencies for every unique word in the tweet.</li>
        </ol>
        <img src="./images/feature-extraction-calculus.png" alt="Calculus">
        <p>This transforms a long, sparse vector into a very small, dense vector like $[1, 8, 11]$, which is much more efficient for