<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Dense Neural Network - Notes</title>

    <link rel="stylesheet" href="../../styles/main.css">
    <style>
        /* Intentionally left minimal — all styles centralized in styles/main.css */
    </style>

    <link rel="stylesheet" href="../../styles/katex.min.css">
    <script defer src="../../styles/katex.min.js"></script>
    <script defer src="../../styles/auto-render.min.js"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '\\[', right: '\\]', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false}
                ]
            });
        });
    </script>
</head>
<body>
    <div class="container">
        <h3>Introduction</h3>

        <h2>DL vs ML</h2>
        <p>Deep learning is an evolution of traditional Machine Learning. The more high-quality data we have, the better deep learning algorithms will perform. This is not always the case for traditional ML, which can stagnate after a certain level.</p>
        <p>Deep learning is based on Artificial Neural Networks (ANN), inspired by how the human brain works.</p>
        <p><strong>ML</strong>: Linear regression / Decision trees<br><strong>DL</strong>: Artificial Neural Networks (ANN)</p>
        <p>ANNs can capture deeper patterns than traditional ML. Deep learning uses multiple layers; the more layers, the deeper the learned representation — hence the name "Deep Learning".</p>
        <img src="./images/deep-learning-vs-others.png" alt="Deep Learning vs Others">
        <p>Deep learning typically requires significant compute power.</p>

        <h2>Perceptron</h2>
        <p>Neural networks are built from layers. Each layer takes inputs and applies parameters to produce outputs.</p>
        <p>A single neuron (perceptron) has: a weight vector $w$, a bias $b$, and an activation function $f$.</p>
        <p>Dot product: $$ w^\top x \,=\, \sum_{j=1}^{n} w_j x_j $$</p>
        <p>Perceptron output for an input $x$: $$ h \,=\, f(w^\top x + b) $$</p>
        <img src="./images/schema-perceptron.png" alt="Perceptron schema">
        <p>The perceptron is a linear model, most effective when data can be linearly separated.</p>

        <h2>Dot product</h2>
        <p>For a point $x = (x_1, x_2)$ and a vector $w = (w_1, w_2)$:</p>
        <ul>
            <li>If $w^\top x &gt; 0$ → class 1</li>
            <li>If $w^\top x &lt; 0$ → class 0</li>
        </ul>
        <p>The decision boundary is $w^\top x = 0$, separating positive from negative.</p>
        <img src="./images/dot-product.png" alt="Dot product">

        <h2>Linear separability</h2>
        <p>To use dot products for classification, linear separability matters. Example: the Iris dataset using Sepal Width and Sepal Length.</p>
        <p>Goal: find a decision boundary to separate species (Green: Iris setosa, Orange: Iris virginica).</p>
        <img src="./images/iris-dataset-decision-boundary.png" alt="Iris Decision Boundary">
        <p>Separating hyperplane: $$ \{ x \in \mathbb{R}^2 : \langle x, w \rangle = x_1 w_1 + x_2 w_2 = 0 \} $$</p>

        <h2>Loss function</h2>
        <p>We count classification errors for a given $w$.</p>
        <p>Given $n$ points $X = (x_i)_{i=1,\dots,n} \in \mathbb{R}^d$ and labels $Y = (y_i)_{i=1,\dots,n} \in \{0,1\}$.</p>
        <p>Classifier: $$ f(x_i, w) = \mathbf{1}[\langle x_i, w \rangle \ge 0] $$</p>
        <p>Empirical 0/1 loss: $$ g(w,X,Y) = \sum_{i=1}^{n} \mathbf{1}[ f(x_i, w) \ne y_i ] $$</p>
        <p>We minimize this loss to find the best separating hyperplane.</p>
        <img src="./images/tensorflow-keras-lossfunctions.png" alt="Loss functions list">
        <div class="side-by-side">
            <img src="./images/iris-loss-functions-schemas.png" alt="Iris loss functions" style="max-width:45%; height:auto;">
            <img src="./images/iris-loss-functions-schemas-2.png" alt="Iris loss functions 2" style="max-width:45%; height:auto;">
        </div>

        <h2>Activation function</h2>
        <p>Activation functions enable differentiable loss and appropriate outputs.</p>
        <ul>
            <li>Sigmoid</li>
            <li>Tanh</li>
            <li>ReLU (Rectified Linear Unit)</li>
            <li>Leaky ReLU</li>
        </ul>
        <p>Hidden layers: often ReLU / Leaky ReLU.</p>
        <p>Output layer depends on task: binary → Sigmoid; multiclass → Softmax; regression → Linear.</p>
        <img src="./images/activation-functions.png" alt="Activation functions">

        <h2>Gradient descent</h2>
        <p>With differentiable activations, we can use gradient descent to optimize $w$.</p>
        <p>Example: if $f(x)=x^2$, then $f'(x)=2x$.</p>
        <div class="side-by-side">
            <img src="./images/gradient-descent-start.png" alt="Gradient Descent start" style="max-width:45%; height:auto;">
            <img src="./images/gradient-descent-xmin.png" alt="Gradient Descent xmin" style="max-width:45%; height:auto;">
        </div>
        <p>Learning rate $\\lambda$ controls step size. Iterate until $|f'(x_k)| &lt; {tol}$.</p>
        <p>Update rule: $$ x_{k+1} = x_k - \lambda f'(x_k) $$</p>
        <img src="./images/convex-gradient-descent.png" alt="Convex Gradient Descent">
        <p>Limitations: guarantees a global minimum only for convex functions. In DL, loss is non-convex; gradient methods often reach a local minimum.</p>
        <p>Therefore, the learning rate is a critical hyperparameter for DL model performance.</p>

        <h3>Predictions with neural networks on tabular data</h3>

        <h2>Context</h2>
        <p>We will train a multilayer perceptron on the Iris dataset (3 species). The goal is to classify the species from tabular features.</p>

        <h2>Multilayer perceptron (MLP)</h2>
        <p>An MLP is a sequence of layers where each layer’s input is the previous layer’s output. For three layers and input $x$:</p>
        <p>$$ H_1 = \text{Layer}_1(x),\quad H_2 = \text{Layer}_2(H_1),\quad O = \text{Layer}_3(H_2). $$</p>
        <img src="./images/layers-density.png" alt="Layers density">
        <p>API styles:</p>
        <ul>
            <li><strong>Sequential</strong>: linear stack of layers (simple networks)</li>
            <li><strong>Functional</strong>: flexible graphs (multiple inputs/outputs, merges)</li>
        </ul>

        <h2>Training pipeline</h2>
        <ol>
            <li><strong>Architecture definition</strong>: choose number of layers/neurons and data flow</li>
            <li><strong>Compilation</strong>: loss, metrics, optimizer (e.g., Adam)</li>
        </ol>
        <pre><code class="language-python">model.compile(
    loss="name_loss_function",
    optimizer="name_optimizer",
    metrics=["name_metric"],
)
        </code></pre>
        <ol start="3">
            <li><strong>Training</strong>: run fit across epochs</li>
        </ol>
        <pre><code class="language-python">model.fit(
    X_train, y_train,
    validation_split=p,
    epochs=nb_epochs,
    batch_size=batch_size,
)
        </code></pre>
        <p><strong>Key notions</strong>: <code>batch_size</code> (samples per update); batch → model → prediction → loss → weight update; <code>epochs</code> (full passes over data).</p>
        <img src="./images/batch-size-illustration.png" alt="Batch size illustration">
        <p><code>validation_split</code> holds out a portion of training data; evaluation runs at each epoch end.</p>

        <h2>Sequential model with Keras</h2>
        <pre><code class="language-python">from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(units=64, activation="relu", input_shape=(784,)))
model.add(Dense(units=10, activation="softmax"))

model.fit(X, y, epochs=nb_epochs, batch_size=batch_size, validation_split=p)
        </code></pre>

        <h2>Model performance</h2>
        <p>Use a confusion matrix on the test set. Convert probabilities to class IDs with NumPy <code>argmax</code> for scikit‑learn reports. For multi‑class tasks, a common loss is <code>sparse_categorical_crossentropy</code>.</p>
    </div>
</body>
</html>
